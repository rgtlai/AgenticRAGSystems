{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mJ9XHTOPUCav"
   },
   "source": [
    "# Deep Dive Agentic Retrieval Augmented Generation\n",
    "\n",
    "An Agentic RAG is required when we use reasoning to determine which action(s) to take and in which order to take them. Essentially we use agents instead of a LLM directly to accomplish a set of tasks which requires planning, multi step reasoning, tool use and/or learning over time. Agents give us agency!\n",
    "\n",
    "Agency : The ability to take action or to choose what action to take\n",
    "\n",
    "In the context of RAG, we can plug in agents to enhance the reasoning prior to selection of RAG pipelines, within a RAG pipeline for retrieval or reranking and finally for synthesising before we send out the response. This improves RAG to a large extent by automating complex workflows and decisions that are required for a non trivial RAG use case.\n",
    "\n",
    "### Purpose of this Agentic RAG\n",
    "This notebook presents a practical implementation of Agentic Retrieval-Augmented Generation (RAG)‚Äîa system where decision-making and tool selection are delegated to an intelligent agent before executing a response. Rather than passing every query through a static RAG pipeline, this system introduces agency‚Äîthe ability to choose the best course of action depending on the nature of the query.\n",
    "\n",
    "At the heart of this implementation is a router prompt, which classifies user queries into one of three categories:\n",
    "\n",
    "- OpenAI documentation: Queries related to tools, APIs, or usage guidelines for OpenAI models\n",
    "- 10-K financial reports: Questions requiring retrieval from company filings or financial datasets\n",
    "- Live Internet search: Broader, current, or comparative queries that need web access\n",
    "\n",
    "Once the query is classified, the system invokes a corresponding route handler:\n",
    "\n",
    "- For OpenAI and 10-K queries, it retrieves relevant context from a vector database (Qdrant) using text embeddings, then applies a RAG-based response generator.\n",
    "- For Internet queries, it fetches real-time information using a web-access API (ARES).\n",
    "\n",
    "This approach is an example of Agentic RAG, where reasoning precedes retrieval and generation. By plugging in agents before and within the RAG pipeline, we make the system smarter and more adaptive. This allows us to:\n",
    "\n",
    "- Automatically choose the right retrieval method based on context\n",
    "- Combine structured knowledge with real-time search\n",
    "- Scale RAG beyond trivial use cases by integrating multi-step decision logic\n",
    "\n",
    "Importantly, no external agentic frameworks are used‚Äîthis is a ground-up implementation that demonstrates how to build a lightweight but intelligent agentic system using only a language model, prompt engineering, and retrieval tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "haelye0PUbdX"
   },
   "source": [
    "## Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HciDI6OpSKJN",
    "outputId": "46bf56b2-9639-4363-8574-53a3a66a972f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install the necessary libraries\n",
    "#!pip install openai\n",
    "#!pip install qdrant_client\n",
    "#!pip install transformers\n",
    "#!pip install -q -U bitsandbytes\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "J7xIyE7iyI63"
   },
   "outputs": [],
   "source": [
    "# Import basic libraries\n",
    "import requests             # Used for making HTTP requests (e.g., calling ARES API for live internet queries)\n",
    "import json                 # For parsing and structuring JSON data (especially OpenAI and routing responses)\n",
    "\n",
    "# Google Colab-specific (for securely handling API keys)\n",
    "#from google.colab import userdata  # To securely store and retrieve credentials in Colab\n",
    "\n",
    "# OS operations\n",
    "import os                   # Useful for accessing environment variables and managing paths\n",
    "\n",
    "# OpenAI API client\n",
    "from openai import OpenAI   # Official OpenAI client library to interface with GPT models for routing and generation\n",
    "\n",
    "# Text processing\n",
    "import re                   # Regular expressions for cleaning or preprocessing inputs (if needed)\n",
    "\n",
    "# Optional visualization (for analysis/debugging purposes)\n",
    "import matplotlib.pyplot as plt       # For displaying charts or visual debug outputs (e.g., embeddings visualizations)\n",
    "import matplotlib.image as mpimg      # For loading/displaying images if needed (rare in RAG, but helpful in demos)\n",
    "\n",
    "# Embedding models (used for text vectorization during retrieval)\n",
    "from transformers import AutoTokenizer, AutoModel  # For loading custom transformer models if not using OpenAI embeddings\n",
    "\n",
    "# Vector database client\n",
    "from qdrant_client import QdrantClient   # Qdrant is used as the vector store to retrieve documents based on similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JE6Bf6r19qhn"
   },
   "source": [
    "## 1. Defining the Internet Tool\n",
    "\n",
    "First, we will define a tool function that enables our system to answer queries requiring real-time, internet-based information. Not all questions can be answered using static documents like OpenAI docs or financial filings‚Äîsometimes users ask about current trends, comparisons, or live updates.\n",
    "\n",
    "To handle this, we introduce a live search capability using the **ARES API** by Traversaal.\n",
    "\n",
    "### What is ARES API?  \n",
    "ARES is a web-based tool that allows you to:\n",
    "\n",
    "- Search the internet in real time.\n",
    "- Get LLM-generated answers based on live search results.\n",
    "\n",
    "This is particularly useful for questions about:\n",
    "\n",
    "- Current events (e.g., *‚ÄúLatest AI tools in 2025‚Äù*),\n",
    "- Tech comparisons (e.g., *‚ÄúGemini vs GPT-4‚Äù*),\n",
    "- General knowledge outside internal datasets.\n",
    "\n",
    "Please generate the API key [here](https://api.traversaal.ai)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "1PydVhqpW43B"
   },
   "outputs": [],
   "source": [
    "#loads ares api key from colab secrets\n",
    "ares_api_key=os.getenv('ARES_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "6tF67DJgZ_sz"
   },
   "outputs": [],
   "source": [
    "import requests  # For sending HTTP POST requests to the ARES API\n",
    "\n",
    "def get_internet_content(user_query: str, action: str):\n",
    "    \"\"\"\n",
    "    Fetches a response from the internet using ARES-API based on the user's query.\n",
    "\n",
    "    This function serves as the tool invoked when the router classifies a query\n",
    "    as requiring real-time information beyond internal datasets‚Äîi.e., \"INTERNET_QUERY\".\n",
    "    It sends the query to a live search API (ARES) and returns the result.\n",
    "\n",
    "    Args:\n",
    "        user_query (str): The user's question that needs a live answer.\n",
    "        action (str): Route type (always expected to be \"INTERNET_QUERY\").\n",
    "\n",
    "    Returns:\n",
    "        str: Response text generated using internet search or an error message.\n",
    "    \"\"\"\n",
    "    print(\"Getting your response from the internet üåê ...\")\n",
    "\n",
    "    # API endpoint for the ARES live search tool\n",
    "    url = \"https://api-ares.traversaal.ai/live/predict\"\n",
    "\n",
    "    # Payload structure expected by the ARES API\n",
    "    payload = {\"query\": [user_query]}\n",
    "\n",
    "    # Authentication and content headers for API access\n",
    "    headers = {\n",
    "        \"x-api-key\": ares_api_key,  # Your secret API key (should be securely loaded from environment)\n",
    "        \"content-type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Send the query to the ARES API and check for success\n",
    "        response = requests.post(url, json=payload, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Extract and return the main response text from the API's nested JSON\n",
    "        return response.json().get('data', {}).get('response_text', \"No response received.\")\n",
    "\n",
    "    # Handle HTTP-level errors (e.g., 400s or 500s)\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        return f\"HTTP error occurred: {http_err}\"\n",
    "\n",
    "    # Handle general connection, timeout, or request formatting issues\n",
    "    except requests.exceptions.RequestException as req_err:\n",
    "        return f\"Request error occurred: {req_err}\"\n",
    "\n",
    "    # Catch-all for any unexpected failure\n",
    "    except Exception as err:\n",
    "        return f\"An unexpected error occurred: {err}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "USRVlUIVaRGM",
    "outputId": "d57bb15f-07e1-4222-8d5b-93e8ecb2c973"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting your response from the internet üåê ...\n",
      "**Top Travel Destinations in 2025**\n",
      "\n",
      "Here's a comprehensive list of the top travel destinations in 2025:\n",
      "\n",
      "**United States:**\n",
      "\n",
      "* Gualala, California\n",
      "* Michigan's M-22 Scenic Drive\n",
      "* Sedona, Arizona\n",
      "* National Parks of the West, Wyoming and Montana\n",
      "* Stinson Beach in California\n",
      "\n",
      "**International Destinations:**\n",
      "\n",
      "1. Sardinia\n",
      "2. Bruges, Belgium\n",
      "3. Crete, Washington\n",
      "4. Osaka, Japan (named No. 1 Trending Destination in the World for 2025)\n",
      "\n",
      "**Other Recommended Destinations:**\n",
      "\n",
      "5. Andros, Greece\n",
      "6. Ronda, Spain\n",
      "7. Aizuwakamatsu, Japan\n",
      "8. Turin, Italy\n",
      "9. Sri Lanka\n",
      "10. Fraser Island, Australia\n",
      "11. Ahr Valley, Germany\n",
      "12. Alaska, US\n",
      "13. Cuba\n",
      "14. Djerba, Tunisia\n",
      "\n",
      "**Budget-Friendly Destinations:**\n",
      "\n",
      "* Asia, particularly Thailand, Malaysia, Vietnam, Laos, and Cambodia\n",
      "\n",
      "**TripAdvisor's Travelers' Choice Awards Best of the Best 2025:**\n",
      "\n",
      "* Osaka, Japan, is named No. 1 Trending Destination in the World for 2025\n",
      "\n",
      "**Lonely Planet's Best in Travel 2025:**\n",
      "\n",
      "* A comprehensive list of the top countries, regions, and cities around the world, chosen by Lonely Planet experts.\n",
      "\n",
      "**Cond√© Nast Traveler's Best Places to Go in 2025:**\n",
      "\n",
      "* Ahr Valley, Germany\n",
      "* Alaska, US\n",
      "* Cuba\n",
      "* Djerba, Tunisia\n",
      "\n",
      "**New York Times' 52 Places to Go in 2025:**\n",
      "\n",
      "* A comprehensive list of the top 52 destinations to visit in 2025.\n",
      "\n",
      "**Reddit's 2025 Biggest Bang for Your Bucks Travel Destination:**\n",
      "\n",
      "* Asia, particularly Thailand, Malaysia, Vietnam, Laos, and Cambodia.\n",
      "\n",
      "**YouTube's Top 10 Places To Visit in 2025 (Year of Travel):**\n",
      "\n",
      "* A travel guide featuring some of the best places to visit in 2025.\n"
     ]
    }
   ],
   "source": [
    "print(get_internet_content(\"Tell me about best travel destinations in 2025?\",\"INTERNET_QUERY\")) #run internet function to test results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ocV_ALVEaRi0"
   },
   "source": [
    "## 2. Router Query Function ‚Äî Giving the Agent Its Brain\n",
    "\n",
    "In this step, we will define the router function, which plays a critical role in our Agentic RAG system.\n",
    "\n",
    "### What is a Router?\n",
    "\n",
    "A router is like the decision-making brain of our assistant.\n",
    "\n",
    "Before trying to answer a user's question, the system first needs to figure out:\n",
    "\n",
    "> ‚ÄúWhere should I go to find the right answer?‚Äù\n",
    "\n",
    "To make this decision, we use the OpenAI GPT model. We provide it with a detailed system prompt that explains how to classify the user's question into one of these categories:\n",
    "\n",
    "- **OPENAI_QUERY** ‚Üí Questions about OpenAI tools, APIs, models, or documentation.\n",
    "- **10K_DOCUMENT_QUERY** ‚Üí Questions about companies, financial filings, or analysis based on 10-K reports.\n",
    "- **INTERNET_QUERY** ‚Üí Anything else that likely requires real-time or general web information.\n",
    "\n",
    "### What does the function do?\n",
    "\n",
    "- Sends the user's question to the OpenAI API.\n",
    "- Receives a JSON response containing:\n",
    "  - `action`: The category the query belongs to.\n",
    "  - `reason`: A short explanation for the decision.\n",
    "  - `answer`: (Optional) A quick response if it‚Äôs simple enough (left blank for internet queries).\n",
    "- Parses the response and returns it as a Python dictionary.\n",
    "\n",
    "### Why is this important?\n",
    "\n",
    "This router gives the system agency‚Äîthe ability to decide which knowledge source to use. It‚Äôs what makes this pipeline agentic, not just static.\n",
    "\n",
    "Without the router, every query would follow the same path. With it, we can:\n",
    "\n",
    "- Dynamically switch between tools and data sources.\n",
    "- Handle different types of user questions intelligently.\n",
    "- Avoid wasting resources on unnecessary steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Bce4h1DG1mW8"
   },
   "outputs": [],
   "source": [
    "# Securely retrieve the OpenAI API key from Colab's user data store\n",
    "# This avoids hardcoding sensitive credentials directly in the notebook\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Initialize the OpenAI client with the retrieved API key\n",
    "# This client will be used for:\n",
    "# - Query classification via the router prompt\n",
    "# - Potentially generating responses from retrieved context\n",
    "openaiclient = OpenAI(api_key=openai_api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "5bBt3L8qijeb"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAIError\n",
    "\n",
    "def route_query(user_query: str):\n",
    "    router_system_prompt =f\"\"\"\n",
    "    As a professional query router, your objective is to correctly classify user input into one of three categories based on the source most relevant for answering the query:\n",
    "    1. \"OPENAI_QUERY\": If the user's query appears to be answerable using information from OpenAI's official documentation, tools, models, APIs, or services (e.g., GPT, ChatGPT, embeddings, moderation API, usage guidelines).\n",
    "    2. \"10K_DOCUMENT_QUERY\": If the user's query pertains to a collection of documents from the 10k annual reports, datasets, or other structured documents, typically for research, analysis, or financial content.\n",
    "    3. \"INTERNET_QUERY\": If the query is neither related to OpenAI nor the 10k documents specifically, or if the information might require a broader search (e.g., news, trends, tools outside these platforms), route it here.\n",
    "\n",
    "    Your decision should be made by assessing the domain of the query.\n",
    "\n",
    "    Always respond in this valid JSON format:\n",
    "    {{\n",
    "        \"action\": \"OPENAI_QUERY\" or \"10K_DOCUMENT_QUERY\" or \"INTERNET_QUERY\",\n",
    "        \"reason\": \"brief justification\",\n",
    "        \"answer\": \"AT MAX 5 words answer. Leave empty if INTERNET_QUERY\"\n",
    "    }}\n",
    "\n",
    "    EXAMPLES:\n",
    "\n",
    "    - User: \"How to fine-tune GPT-3?\"\n",
    "    Response:\n",
    "    {{\n",
    "        \"action\": \"OPENAI_QUERY\",\n",
    "        \"reason\": \"Fine-tuning is OpenAI-specific\",\n",
    "        \"answer\": \"Use fine-tuning API\"\n",
    "    }}\n",
    "\n",
    "    - User: \"Where can I find the latest financial reports for the last 10 years?\"\n",
    "    Response:\n",
    "    {{\n",
    "        \"action\": \"10K_DOCUMENT_QUERY\",\n",
    "        \"reason\": \"Query related to annual reports\",\n",
    "        \"answer\": \"Access through document database\"\n",
    "    }}\n",
    "\n",
    "    - User: \"Top leadership styles in 2024\"\n",
    "    Response:\n",
    "    {{\n",
    "        \"action\": \"INTERNET_QUERY\",\n",
    "        \"reason\": \"Needs current leadership trends\",\n",
    "        \"answer\": \"\"\n",
    "    }}\n",
    "\n",
    "    - User: \"What's the difference between ChatGPT and Claude?\"\n",
    "    Response:\n",
    "    {{\n",
    "        \"action\": \"INTERNET_QUERY\",\n",
    "        \"reason\": \"Cross-comparison of different providers\",\n",
    "        \"answer\": \"\"\n",
    "    }}\n",
    "\n",
    "    Strictly follow this format for every query, and never deviate.\n",
    "    User: {user_query}\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Query the GPT-4 model with the router prompt and user input\n",
    "        response = openaiclient.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"system\", \"content\": router_system_prompt}]\n",
    "        )\n",
    "\n",
    "        # Extract and parse the model's JSON response\n",
    "        task_response = response.choices[0].message.content\n",
    "        json_match = re.search(r\"\\{.*\\}\", task_response, re.DOTALL)\n",
    "        json_text = json_match.group()\n",
    "        parsed_response = json.loads(json_text)\n",
    "        return parsed_response\n",
    "\n",
    "    # Handle OpenAI API errors (e.g., rate limits, authentication)\n",
    "    except OpenAIError as api_err:\n",
    "        return {\n",
    "            \"action\": \"INTERNET_QUERY\",\n",
    "            \"reason\": f\"OpenAI API error: {api_err}\",\n",
    "            \"answer\": \"\"\n",
    "        }\n",
    "\n",
    "    # Handle case where model response isn't valid JSON\n",
    "    except json.JSONDecodeError as json_err:\n",
    "        return {\n",
    "            \"action\": \"INTERNET_QUERY\",\n",
    "            \"reason\": f\"JSON parsing error: {json_err}\",\n",
    "            \"answer\": \"\"\n",
    "        }\n",
    "\n",
    "    # Catch-all for any other unforeseen issues\n",
    "    except Exception as err:\n",
    "        return {\n",
    "            \"action\": \"INTERNET_QUERY\",\n",
    "            \"reason\": f\"Unexpected error: {err}\",\n",
    "            \"answer\": \"\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ilSsVBBv-KT",
    "outputId": "a5a88b94-e667-4286-e48e-b43156e4ce34"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'action': 'INTERNET_QUERY',\n",
       " 'reason': 'Query requires specific 2021 data',\n",
       " 'answer': ''}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "route_query(\"what is the revenue of uber in 2021?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VrkE-V4PlClH"
   },
   "source": [
    "## 3. Setting Up Qdrant Vector Database for Agentic RAG\n",
    "In this step, we are connecting our agent to a pre-built vector database using Qdrant‚Äîa tool used to store and search document embeddings (numerical representations of text).\n",
    "\n",
    "What Are We Doing?\n",
    "We are loading an existing Qdrant database that was downloaded from a GitHub repository. This database already contains:\n",
    "\n",
    "- Vectorized OpenAI documentation\n",
    "- Vectorized 10-K financial filings\n",
    "\n",
    "By loading this saved data:\n",
    "\n",
    "- We save time (no need to re-embed the documents)\n",
    "- We enable fast similarity search to retrieve relevant text chunks\n",
    "\n",
    "This setup allows our system to perform semantic search, meaning it can understand the meaning of the user query and match it with the most relevant pieces of information stored in the database.\n",
    "\n",
    "\n",
    "### Why This Matters in Agentic RAG\n",
    "Once the router decides that the query should go to the OpenAI docs or the 10-K reports, our system uses Qdrant to:\n",
    "\n",
    "- Search for the most relevant pieces of text\n",
    "- Pass those to the model to generate a grounded answer\n",
    "\n",
    "So, this step is essential to support retrieval-augmented generation (RAG) within our agentic flow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yXAXLB2eCgSn"
   },
   "source": [
    "#Data Sources:\n",
    "\n",
    "**10K Database: Lyft 2024 & Uber 2021 SEC filings**\n",
    "\n",
    "**OpenAI Docs: Official OpenAI documentation**\n",
    "\n",
    "For lecture demo purposes, the vecitr database has already been created and hosted on Github which we will clone here. In order to create your own embeddings, the notebook and data will be hosted and shared on github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "suqlk9P9YJfQ",
    "outputId": "cebf146c-32e6-449f-dcbb-bca674625ef0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'multi-agent-course' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "# Clone the project repository that contains prebuilt vector data (e.g., Qdrant collections)\n",
    "# This includes document embeddings and configurations needed for retrieval (10-K, OpenAI docs)\n",
    "!git clone https://github.com/hamzafarooq/multi-agent-course.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "BB-p1nzYo91-"
   },
   "outputs": [],
   "source": [
    "# üóÑÔ∏è Initializing Qdrant client with local path to vector database\n",
    "# The path points to prebuilt Qdrant collections (10-K and OpenAI docs) cloned from the repository\n",
    "# This enables fast, local retrieval of relevant document chunks based on semantic similarity\n",
    "client = QdrantClient(path=\"multi-agent-course/Module_1/Agentic_RAG/qdrant_data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z90HaV0jm93H"
   },
   "source": [
    "## 4. Building the Retriever and RAG for Vector Databases\n",
    "In this section, we build the core logic that allows our agent to find relevant documents and generate grounded answers using them.\n",
    "\n",
    "###Step 1: Import the Embedding Model\n",
    "We start by importing the nomic-ai/nomic-embed-text-v1.5 model from Hugging Face. This model is used to convert any text (such as a user query) into a dense vector, known as an embedding. These embeddings capture the semantic meaning of text, allowing us to later compare and retrieve similar documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 543,
     "referenced_widgets": [
      "90174ea8461b45d783da06abbbf971c4",
      "c6def64dfaaa4cd096bbf66288a3cb98",
      "ba227c8c08074af69fdf6686b5efbbbc",
      "95e2fc4b397c47418c653e0e5df7f850",
      "f7029f9197d448b9b633e1eab6ad16d1",
      "fac0922ead2c4b238828326d33d79280",
      "9f61e80372d5407cabab74591531d840",
      "c5fee6518ab94acc905a065c2b66accd",
      "c9b17797e50b4bafb31c3b4c9d11f530",
      "2b286ec3d35745feb5d3619d7a33255b",
      "bf0345d22a474cc88364c7ae085ba341",
      "12ecc8da0d1b479fa85b7b3067cfa123",
      "0e2bbe94c92d44f8ba53b47a6610a1cb",
      "b1bafb42b1dd4bde89784d9271f412c5",
      "e73972c1cd4646eea672c1381a8afa14",
      "63ba7123943146e797daf94b0577c560",
      "6888e71170944f089657a5e3d84b6e67",
      "1d56719e1d80440bb96ce4b6b850b2f5",
      "b531725a7cb0421881aadef8d070e39b",
      "9fb33dcc0970484183999449db51a2ff",
      "3a1fe1851d7943dab8ad10ffda8623a0",
      "897883c5baa34b1bb36b9bb165be1a70",
      "959aa53853554f59a9fecda26387a741",
      "bc44f96a21da40bb8159363db99341f2",
      "57405bbb3a964a0a916797e13dba4ac1",
      "ede92ed17d8c4148bb16923cc5a3df94",
      "e0787953e0d546a0b33a40daa2462ea4",
      "a6e7849900084933999169f00c68450e",
      "d8d9fc1715f9446085e63a4efc5c171b",
      "795edfb6b0854759a992a695ede81233",
      "bc84c2c018fb49ec9751b0fd2d6249c0",
      "6108f5bf04ce4987a907573789e35c04",
      "0837e125ca8a4026bf84779d43391136",
      "904e8b59a3984f6c8296f3897981b566",
      "7cee7c933139450faa720b14b5886143",
      "d46092d6573d4d35b7004d8adc833b5b",
      "39993c45d73f46f4bc68f4bb307e5b8a",
      "52745c7638e746b799c72f47ff428cfd",
      "c9b4d5ae2e2149a3b3389600bf1dfeb6",
      "79d639a3ae9f4180a80754d474f98cd4",
      "0280dc48351e438f9e8e01368542aa3f",
      "fabd05cae1d24367b1bb213bb828b94f",
      "01887908d11d4bd68301d5269428aeda",
      "808d4cf6be3942319972b0a894e21cf3",
      "291c13ff85234fb69307b60351cc188f",
      "ab910d9cd28041b9afc541a2add59a62",
      "4c02cf4d7ef34eaa86288fb3157e63db",
      "4b4b4a3bb99149aba1eb7cdb8b322d42",
      "bf183304dac146ccad50ee97afcc7c83",
      "5a2993a04b414e119e57c51d0854c84f",
      "2f018f6e751f4bdc8d9c636d1b10b43e",
      "f47d8e8fedc34484ad7b7aa7b651aaf2",
      "ce925cb3b56a444b82ef4dc68195c974",
      "318c31602e414fae8af72b670ea8adc0",
      "a7dbcc1a47ca4e10a3731ecfe4f9da99",
      "101987c033de4e5789a993c4ff18dcc1",
      "94f6caa631c449318f77e17facc05557",
      "2f1f6fe523314b9fa96965c37b577058",
      "3e2680aeda7f43809fe56c5436a5b8a6",
      "a2ea72c3cc3f4eeb8e2abd0ff2a3c650",
      "9cbe6858557f47cb960faa3b7e074262",
      "cd55198f700d48faa9c00336cd8da52b",
      "f42dfeebef024612bd8161fb14790160",
      "a0bd52350bae4041aee21fadec60f2a0",
      "24fad4b5411e46439f2f151002684e87",
      "e2beaceeb68645668664c380e737d474",
      "e1306961bba04c458cd5cf0e24a545f8",
      "34dce831203c4289974676d4a1f9b25d",
      "d073f408d69b4c218a7a8b0bae66eedb",
      "82219639322c4c00bd6523303ecb96f3",
      "eff190f5486a4d5fa37b4404cbcf9608",
      "b46636ce4cc44fd4b4a64a92dc0ee849",
      "86f9a0f89be6452391c1d2ee667e571a",
      "9e4534aae1b8458686e5b91e832e3d18",
      "59410c4ca1b14d98aa5eb3fbeed9b326",
      "cc1404c871ff4cafb1d2cea9d91e4b54",
      "458d17f9f31846d7b36504b13270874c",
      "72d1627c25f0404db71824371969a63e",
      "50d4a18343174a2faa9d72cc43d9261d",
      "2192d7d45c1b458e99b2035ab820444e",
      "4459dab643b54b8493f040085944bb55",
      "101e25e6fda248aca96918173c5173b4",
      "33bf9b222abc4806a41da0402ce4aae0",
      "916ccfa795aa4fbc8e3280ccf6a056d2",
      "a3a26ce9a762419e86384a03837a9f86",
      "f3838254c57d4212ac0fa0a91add2733",
      "972e5d2db4ef4fa786a0cdbd606e694f",
      "3c0eab0ba2464d0c9c541c97f2195ab1"
     ]
    },
    "id": "qM6mDo0InEji",
    "outputId": "a07c11bc-bb21-456b-d410-adcb0cd7e96a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.2799689   0.40158418 -3.5162663  -0.3981327   1.5919126 ]\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer and embedding model from Hugging Face\n",
    "# This model converts raw text into dense vector representations (embeddings)\n",
    "# Used for similarity search in Qdrant during document retrieval\n",
    "text_tokenizer = AutoTokenizer.from_pretrained(\"nomic-ai/nomic-embed-text-v1.5\", trust_remote_code=True)\n",
    "text_model = AutoModel.from_pretrained(\"nomic-ai/nomic-embed-text-v1.5\", trust_remote_code=True)\n",
    "\n",
    "def get_text_embeddings(text):\n",
    "    \"\"\"\n",
    "    Converts input text into a dense embedding using the Nomic embedding model.\n",
    "    These embeddings are used to query Qdrant for semantically relevant document chunks.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text or query from the user.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A fixed-size vector representing the semantic meaning of the input.\n",
    "    \"\"\"\n",
    "    # Tokenize and prepare input for the model\n",
    "    inputs = text_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    # Forward pass to get model outputs\n",
    "    outputs = text_model(**inputs)\n",
    "\n",
    "    # Take the mean across all token embeddings to get a single vector (pooled representation)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "    # Convert to NumPy array and detach from computation graph\n",
    "    return embeddings[0].detach().numpy()\n",
    "\n",
    "# Example usage: Generate and preview the embedding of a test sentence\n",
    "text = \"This is a test sentence.\"\n",
    "embeddings = get_text_embeddings(text)\n",
    "print(embeddings[:5])  # Print first 5 dimensions for inspection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nXOx47EKw2ir"
   },
   "source": [
    "### Step 2: Define the Embedding Function\n",
    "We then define a function get_text_embeddings() which:\n",
    "\n",
    "- Tokenizes the input text\n",
    "- Runs it through the model\n",
    "- Computes the average of all token embeddings\n",
    "- Returns a single vector that represents the full sentence\n",
    "\n",
    "This vector will be used to query Qdrant to find the most relevant document chunks based on similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "3r6MeyRz8sv7"
   },
   "outputs": [],
   "source": [
    "def rag_formatted_response(user_query: str, context: list):\n",
    "    \"\"\"\n",
    "    Generate a response to the user query using the provided context,\n",
    "    with article references formatted as [1][2], etc.\n",
    "\n",
    "    This function performs the final step in the RAG pipeline‚Äîsynthesizing an answer\n",
    "    from retrieved document chunks (context). It prompts the model to generate a\n",
    "    grounded response, explicitly citing sources using a reference format.\n",
    "\n",
    "    Args:\n",
    "        user_query (str): The user's original question.\n",
    "        context (list): List of text chunks retrieved from Qdrant (10-K or OpenAI docs).\n",
    "\n",
    "    Returns:\n",
    "        str: A generated response grounded in the retrieved context, with numbered citations.\n",
    "    \"\"\"\n",
    "\n",
    "    # Construct a RAG prompt that includes both:\n",
    "    # 1. The user's query\n",
    "    # 2. The supporting context documents\n",
    "    # The prompt instructs the model to answer using only the provided context,\n",
    "    # and to include citations like [1], [2], etc. based on chunk IDs or order.\n",
    "    rag_prompt = f\"\"\"\n",
    "       Based on the given context, answer the user query: {user_query}\\nContext:\\n{context}\n",
    "       and employ references to the ID of articles provided [ID], ensuring their relevance to the query.\n",
    "       The referencing should always be in the format of [1][2]... etc. </instructions>\n",
    "    \"\"\"\n",
    "\n",
    "    #  Call GPT-4o to generate the response using the RAG-style prompt\n",
    "    response = openaiclient.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": rag_prompt},\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Return the model's generated answer\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zdaeGcoCDw5w"
   },
   "source": [
    "### Step 3: Define the RAG Response Generator\n",
    "After retrieving relevant text chunks from Qdrant, we use the rag_formatted_response() function to generate a final answer. This function:\n",
    "\n",
    "- Takes the user query and the retrieved document chunks\n",
    "- Builds a prompt that asks the language model (GPT-4o) to answer the question using only the provided context\n",
    "- Instructs the model to include references like [1], [2] for traceability\n",
    "\n",
    "This ensures the output is not only informative but also grounded in actual retrieved data.\n",
    "\n",
    "Together, these two functions lay the foundation for combining retrieval (from vector DB) and generation (from LLM) ‚Äî the two pillars of a RAG system.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "hp0y2J1AgX2t"
   },
   "outputs": [],
   "source": [
    "def retrieve_and_response(user_query: str, action: str):\n",
    "    \"\"\"\n",
    "    Retrieves relevant text chunks from the appropriate Qdrant collection\n",
    "    based on the query type, then generates a response using RAG.\n",
    "\n",
    "    This function powers the retrieval and response generation pipeline\n",
    "    for queries that are classified as either OPENAI-related or 10-K related.\n",
    "    It uses semantic search to fetch relevant context from a Qdrant vector store\n",
    "    and then generates a response using that context via a RAG prompt.\n",
    "\n",
    "    Args:\n",
    "        user_query (str): The user's input question.\n",
    "        action (str): The classification label from the router (e.g., \"OPENAI_QUERY\", \"10K_DOCUMENT_QUERY\").\n",
    "\n",
    "    Returns:\n",
    "        str: A model-generated response grounded in retrieved documents, or an error message.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define mapping of routing labels to their respective Qdrant collections\n",
    "    collections = {\n",
    "        \"OPENAI_QUERY\": \"opnai_data\",           # Collection of OpenAI documentation embeddings\n",
    "        \"10K_DOCUMENT_QUERY\": \"10k_data\"        # Collection of 10-K financial document embeddings\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Ensure that the provided action is valid\n",
    "        if action not in collections:\n",
    "            return \"Invalid action type for retrieval.\"\n",
    "\n",
    "        # Step 1: Convert the user query into a dense vector (embedding)\n",
    "        try:\n",
    "            query = get_text_embeddings(user_query)\n",
    "        except Exception as embed_err:\n",
    "            return f\"Embedding error: {embed_err}\"  # Fail early if embedding fails\n",
    "\n",
    "        # Step 2: Retrieve top-matching chunks from the relevant Qdrant collection\n",
    "        try:\n",
    "            text_hits = client.query_points(\n",
    "                collection_name=collections[action],  # Choose the right collection based on routing\n",
    "                query=query,                          # The embedding of the user's query\n",
    "                limit=3                               # Fetch top 3 relevant chunks\n",
    "            ).points\n",
    "        except Exception as qdrant_err:\n",
    "            return f\"Vector DB query error: {qdrant_err}\"  # Handle Qdrant access issues\n",
    "\n",
    "        # Extract the raw content from the retrieved vector hits\n",
    "        contents = [point.payload['content'] for point in text_hits]\n",
    "\n",
    "        # If no relevant content is found, return early\n",
    "        if not contents:\n",
    "            return \"No relevant content found in the database.\"\n",
    "\n",
    "        # Step 3: Pass the retrieved context to the RAG model to generate a response\n",
    "        try:\n",
    "            response = rag_formatted_response(user_query, contents)\n",
    "            return response\n",
    "        except Exception as rag_err:\n",
    "            return f\"RAG response error: {rag_err}\"  # Handle generation failures\n",
    "\n",
    "    # Catch any unforeseen errors in the overall process\n",
    "    except Exception as err:\n",
    "        return f\"Unexpected error: {err}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yfYzErTYzhnK"
   },
   "source": [
    "# 5. Putting It All Together: Running the Agentic RAG\n",
    "In this final step, we combine everything into a single function that controls the entire Agentic RAG workflow. The agentic_rag() function acts as the main orchestrator of the system.\n",
    "\n",
    "Here‚Äôs what it does:\n",
    "\n",
    "- Prints the user's query for reference.\n",
    "- Uses the router function (powered by GPT) to decide which type of data source to use:\n",
    "  - OpenAI documentation\n",
    "  - 10-K financial reports\n",
    "- Internet search\n",
    "- Calls the correct function based on the route:\n",
    "- If it‚Äôs an OpenAI or 10-K query, it retrieves data from Qdrant and generates a RAG response.\n",
    "- If it‚Äôs an Internet query, it uses the ARES API to fetch live information.\n",
    "- Displays the final response, neatly formatted in the console.\n",
    "\n",
    "This step brings the agentic loop full circle‚Äîfrom understanding the question, reasoning about where to search, to finally responding with the best possible answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ssG-BM9GpT-p"
   },
   "outputs": [],
   "source": [
    "# Dictionary that maps the route labels (decided by the router) to their respective functions\n",
    "# Each type of query is handled differently:\n",
    "# - OPENAI_QUERY and 10K_DOCUMENT_QUERY use document retrieval + RAG\n",
    "# - INTERNET_QUERY uses a web search API\n",
    "routes = {\n",
    "    \"OPENAI_QUERY\": retrieve_and_response,\n",
    "    \"10K_DOCUMENT_QUERY\": retrieve_and_response,\n",
    "    \"INTERNET_QUERY\": get_internet_content,\n",
    "}\n",
    "\n",
    "def agentic_rag(user_query: str):\n",
    "    \"\"\"\n",
    "    Main function that runs the full Agentic RAG system.\n",
    "\n",
    "    This function takes a user's question, decides what type of query it is (OpenAI-related,\n",
    "    financial document-related, or general internet), and then calls the right function\n",
    "    to handle it. Finally, it prints out the full conversation and response.\n",
    "\n",
    "    Args:\n",
    "        user_query (str): The user's input question.\n",
    "\n",
    "    Returns:\n",
    "        None (It just prints the result nicely to the console)\n",
    "    \"\"\"\n",
    "\n",
    "    #  Terminal color codes to make the printed output easier to read and visually structured\n",
    "    CYAN = \"\\033[96m\"\n",
    "    GREY = \"\\033[90m\"\n",
    "    BOLD = \"\\033[1m\"\n",
    "    RESET = \"\\033[0m\"\n",
    "\n",
    "    try:\n",
    "        # Step 1: Print the user's original question to the console\n",
    "        print(f\"{BOLD}{CYAN}üë§ User Query:{RESET} {user_query}\\n\")\n",
    "\n",
    "        # Step 2: Use the router (powered by GPT) to decide which route the query belongs to\n",
    "        try:\n",
    "            response = route_query(user_query)\n",
    "        except Exception as route_err:\n",
    "            # If something goes wrong while classifying the query, show an error message\n",
    "            print(f\"{BOLD}{CYAN}ü§ñ BOT RESPONSE:{RESET}\\n\")\n",
    "            print(f\"Routing error: {route_err}\\n\")\n",
    "            return\n",
    "\n",
    "        # Extract the routing decision and the reason behind it\n",
    "        action = response.get(\"action\")  # e.g., \"OPENAI_QUERY\"\n",
    "        reason = response.get(\"reason\")  # e.g., \"Related to OpenAI tools\"\n",
    "\n",
    "        # Step 3: Show the selected route and why it was chosen\n",
    "        print(f\"{GREY}üìç Selected Route: {action}\")\n",
    "        print(f\"üìù Reason: {reason}\")\n",
    "        print(f\"‚öôÔ∏è Processing query...{RESET}\\n\")\n",
    "\n",
    "        # Step 4: Call the correct function depending on the route (retrieval or web search)\n",
    "        try:\n",
    "            route_function = routes.get(action)  # Find the function to use for this route\n",
    "            if route_function:\n",
    "                result = route_function(user_query, action)  # Run the function with the user's input\n",
    "            else:\n",
    "                result = f\"Unsupported action: {action}\"  # Catch unknown routing types\n",
    "        except Exception as exec_err:\n",
    "            result = f\"Execution error: {exec_err}\"  # Handle failure in the chosen route function\n",
    "\n",
    "        # Step 5: Print the final response to the user\n",
    "        print(f\"{BOLD}{CYAN}ü§ñ BOT RESPONSE:{RESET}\\n\")\n",
    "        print(f\"{result}\\n\")\n",
    "\n",
    "    except Exception as err:\n",
    "        # Catch-all for any unexpected errors in the overall logic\n",
    "        print(f\"{BOLD}{CYAN}ü§ñ BOT RESPONSE:{RESET}\\n\")\n",
    "        print(f\"Unexpected error occurred: {err}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0-rTHJpGDTea",
    "outputId": "975f082d-609f-4dcb-eba1-39ead4126114"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[96müë§ User Query:\u001b[0m what was uber revenue in 2021?\n",
      "\n",
      "\u001b[90müìç Selected Route: 10K_DOCUMENT_QUERY\n",
      "üìù Reason: Query pertains to annual revenue\n",
      "‚öôÔ∏è Processing query...\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[96mü§ñ BOT RESPONSE:\u001b[0m\n",
      "\n",
      "Uber's revenue in 2021 was $17,455 million [1][2].\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agentic_rag(\"what was uber revenue in 2021?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TncVUsVpu3WC",
    "outputId": "361dcae2-c4d6-4dec-baa2-7bfef097af05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[96müë§ User Query:\u001b[0m what was lyft revenue in 2024?\n",
      "\n",
      "\u001b[90müìç Selected Route: INTERNET_QUERY\n",
      "üìù Reason: Future financial data not available\n",
      "‚öôÔ∏è Processing query...\u001b[0m\n",
      "\n",
      "Getting your response from the internet üåê ...\n",
      "\u001b[1m\u001b[96mü§ñ BOT RESPONSE:\u001b[0m\n",
      "\n",
      "Lyft's revenue in 2024 was approximately $5.786 Billion, with a 31.39% increase from 2023.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agentic_rag(\"what was lyft revenue in 2024?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mdb97ckP6y2h",
    "outputId": "10381c21-f258-439b-ad5f-b566ed483ca3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[96müë§ User Query:\u001b[0m List me down new LLMs in 2025\n",
      "\n",
      "\u001b[90müìç Selected Route: INTERNET_QUERY\n",
      "üìù Reason: Requires information beyond 2023\n",
      "‚öôÔ∏è Processing query...\u001b[0m\n",
      "\n",
      "Getting your response from the internet üåê ...\n",
      "\u001b[1m\u001b[96mü§ñ BOT RESPONSE:\u001b[0m\n",
      "\n",
      "List me down new LLMs in 2025:\n",
      "\n",
      "Here is the comprehensive list of new large language models in 2025:\n",
      "\n",
      "1. GPT-4.5 (Orion) - released in 2025\n",
      "2. Claude 3.7 Sonnet - released in 2025\n",
      "3. Gemini 2.5 Pro - released in 2025\n",
      "4. DeepSeek-V3-0324 - released in 2025\n",
      "5. Grok-3 - released in 2025\n",
      "6. Qwen3 - released in 2025\n",
      "7. LLaMA 3 - released in 2025 (developed by Meta)\n",
      "8. LLaMA 4 -  (part of the LLaMA 3 family)\n",
      "9. Qwen -  (listed among the top 9 large language models as of August 2025)\n",
      "10. Mistral - (listed among the top 9 large language models as of August 2025)\n",
      "11. GPT-4.1 - (listed among the 23 best large language models in 2025)\n",
      "12. Claude Sonnet 4 - (Anthropic's newest conversational AI model, released in May 2025)\n",
      "\n",
      "Other notable large language models:\n",
      "\n",
      "- LLaMA family of models\n",
      "- GPT family of models\n",
      "- PaLM ( discussed in the survey paper at arXiv)\n",
      "\n",
      "Note: Information may not be up to date as the query date is September 03, 2025.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agentic_rag(\"List me down new LLMs in 2025\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wpO6W8peh1qO",
    "outputId": "b93c8cad-6149-48df-9fbb-60ad4690afd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[96müë§ User Query:\u001b[0m how to work with chat completions?\n",
      "\n",
      "\u001b[90müìç Selected Route: OPENAI_QUERY\n",
      "üìù Reason: Question about OpenAI's ChatGPT\n",
      "‚öôÔ∏è Processing query...\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[96mü§ñ BOT RESPONSE:\u001b[0m\n",
      "\n",
      "To work with chat completions, a number of steps are involved:\n",
      "\n",
      "1. **Get Chat Completion**: Fetch the stored chat completions by using `GET` endpoint, `https://api.openai.com/v1/chat/completions/{completion_id}/messages`. You can get the ID of the chat completion by retrieving it from the path parameters[1].\n",
      "\n",
      "2. **Retrieve Chat Messages**: Retrieve individual messages from a chat completion via `completion_id`, which is required. This identifier helps in retrieving last message from the previous pagination request[1]. \n",
      "\n",
      "3. **List of Chat Completions**: To list stored chat completions, use `GET https://api.openai.com/v1/chat/completions`. It will only return chat completions that have been stored with the `store` parameter set to true[3].\n",
      "\n",
      "4. **Filter Chat Completions**: You can filter the list of chat completions by metadata keys. For example: `metadata[key1]=value1&metadata[key2]=value2`. This is optional and helps in fetching specific chat completions[3]. \n",
      "\n",
      "5. **Limits and Order**: You can specify the number of chat completions to retrieve using `limit` and also set the order of retrieval using `order`[1]. The default limit is set to 20 and default order of messages is ascending[1].\n",
      "\n",
      "6. **Retrieve Stored Chat Completions**: To retrieve stored chat completions or its messages, ensure the chat completion was created with the `store` parameter set to true[1].\n",
      "\n",
      "7. **Pagination**: If there are more chat completions available than the specified limit, the response will indicate that more data is available using `has_more`[1][8].\n",
      "\n",
      "8. **Stream Chat Completions in Real-time**: You can use server-sent events to receive chunks of completions returned from the model in real-time[8].\n",
      "\n",
      "Please remember to include appropriate headers in your request such as \"Authorization: Bearer $OPENAI_API_KEY\" and \"Content-Type: application/json\"[1][3].\n",
      "\n",
      "For full details and specific parameters, consult the OpenAI API documentation[4].\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agentic_rag(\"how to work with chat completions?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "muK18eZ2L4bQ"
   },
   "source": [
    "# 6. CHANGE TO QUANTIZED LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SOM57QgnNnIr"
   },
   "source": [
    "In previous assignment we created a quantized model and placed it into Hugging Face. We will use this to rerun the above queries. We will use gpt-neox-20b as the quantized LLM. We will rewrite the route_query to route_query_quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "0619bb85205c41afbbbcac96ce0dcfe8",
      "bbb1e873e4384c9faf9d255db25b4685",
      "f28edaf9827e493eb8e063c72a267f87",
      "50b3339bdc0543dfa456186e91724f31",
      "cecc83c5b0974065af0451b603562241",
      "ee9c621ce7e44a4aab6491db64d1566b",
      "6bad4c5bbbed4fe0a7a9224f021e5f0b",
      "a01b7617f1594229b113889e6a54e8ad",
      "7b0c897faf344727bbfe89660ec8c467",
      "71790e10282b4aba93aa5c96be83d610",
      "201409a399ce47248505fce47a178761",
      "ffdd13ee9eb448b1abbde10a2119a5af",
      "c896148b461f4c118d8de05197945fba",
      "96d632bf2da74bf38e4575d019976cc8",
      "2825607e362a4494ae427f6610061a60",
      "51e9d175492d4e4f9b79c9de4f4664e1",
      "4d242e1f212849459c5aa71b86b33e5d",
      "b644556307ad4e9bad2a159e3ca7f10b",
      "da98060f68fa44a599cacc3220a0c99f",
      "3e19981dfc6d4685b9fef2c809e49ec1",
      "8059c38aadc74459a156e9a7bfebec81",
      "1ca8e2579d2e480db9c3c85f80f71413",
      "c75dde99305a4066ae82502ca907f3f4",
      "dc11cfd032e14468ad6dc4ce9fbaf4b9",
      "be7a383a449c4d47b0d725e7008fab3f",
      "262b17384144428782dd6496da267aaa",
      "c5f22c48cbf6422aad1e1f3ca9cff762",
      "dc9c1d649adb49c18814ee04d793b831",
      "57c38038ff05422b9102f9424d02e75a",
      "a6c75460c087400b9fb9436c74dfe4c1",
      "d73ff4b9f5ed44a3824d2f2132ced9dc",
      "13c1341b972d4ee09067b0a2b2531b56",
      "3d69163fe7304a96b919bdbca54a3f20",
      "944f8afb403c4642b30a161d118b4dea",
      "bb717c96d9a843c696083ab23ddb7baf",
      "faef7fc82e904213ab0ac69c6c27a4b8",
      "7b78680f5e664ebb973e8171bfb27b4d",
      "73236242044c45e1928361e488933e42",
      "f060e165dc714327b0565973667b9f49",
      "3a426cdd1dfe4cd6948e318a5513206b",
      "98b41b804f124be2a4f84d04e94d721c",
      "020118b1840f41b5a68e674d764498b2",
      "babec28c2de74c0b845952f5fb17c7ce",
      "9420b7114c5d432788cfcf2504d78e4e",
      "28634577a3914b05b41b1111fbe3d2c4",
      "213ccd527a654378b118bc4e3c9c53fa",
      "ad01039e1b6d4eac8356caa4ca028c73",
      "652394b7509f4e9e934fb61d96804db5",
      "3618b647e5bb440bb04f8e405c14d1fc",
      "ca479a96fe4441d4890e1efdb45ea7c5",
      "b10ff4ad24b54789bb24b1662605f335",
      "7de234d1dbf1450ba1e4c87b5499888c",
      "ed4af59fd779442c8bff88c385ddf0a1",
      "e7af088f43234c569ba834d2ffe89012",
      "9387272420c04863989e81f4f570e8fb",
      "240051c0cf8d40eba54aa759eed05902",
      "851f1c63466e4d6a9df506e1359304d4",
      "52439a33572e475ab67633b349727974",
      "3228badc889648ea85fcd08e7f9a382c",
      "e4a34745795e49ea91707e0872b63939",
      "8c5287f37f144a578d5467736391f321",
      "3a058653e4534da2a62023a88972dc99",
      "1a8c53a061ba4cf4901e86231d914a3c",
      "b8b907bad80e4b478c062ed61a30f600",
      "c63fe336c94849a6bd6c1093583fd49f",
      "9da973715109400e9be39542e97676c5",
      "8b6a15177eb84025b4a3ea642689e860",
      "67e65cb7250d490c86fc2d77bd4d4d72",
      "aa1ca9ebf7254718be6dd548185c7996",
      "a26dd2268a5e4ac88ad4d1ba80f581f5",
      "08bbf534043240fa8546548237c682f9",
      "319f52b849854772ac8f11a9c778f4fd",
      "fd468266d64d46788b24fd52db6e97bd",
      "e80e143d7e854e99a151e4c0fc75956c",
      "1b8b9a1bb86c4906bada9d3460e48923",
      "5b1fc37b4f334910b86828cd9af1cd21",
      "bdd936f25a074663acb74fe9b29898fa",
      "c3dee31bd09c4c878998ee3c091bd9c8",
      "612661783a1e40e1bb0778bb0f833aba",
      "06efc5eb60ee406da1f267d51fb0886d",
      "45ed5da392514712bbd3143bd663488c",
      "20b669aa746d4316ad9cc21a67c6650f",
      "87416b6f9622416f8a699202a2410990",
      "b29962cc643344db8951d86242e4c861",
      "1c1266695351457da0ccae25b2b7d979",
      "923040792a1948f28660475c4d1beb22",
      "331c496a64ab4590a80037c68378a0f4",
      "2ed3b13e03614de98f9d06d0e704e3c9",
      "1060b722b6244b3db1da0c6e617c33b0",
      "efa95e211ea24c969e8cd81a412c68dc",
      "a58af486c41c46f08cd0ecec8542970a",
      "db44ff0e713144719df495781da14f86",
      "64a54ae9a5cf4d649fea48dcf41839b5",
      "1584bddd2fc54ce48188adf5e81a0fbd",
      "3bc1b8f10dca4d2fafe50204e299c6c2",
      "2acce0e2e91f476fb327d6577816f1db",
      "882a50ebafca4f7daf083fb3f416a01e",
      "b597620854e64837a6be1ad790929699",
      "40285ffe03fb421fa60584be513b97ca",
      "4d2a8654f54443339b4ee2acafd0f78b",
      "54755e824d364bb1aa6300e5f017fd6e",
      "6b2c2a3e5e1e46deab9484e1436455f4",
      "284ec7b220784c5eae2f3b96dce4bee9",
      "660c6710162a4523a004ed9ef3447414",
      "dad7c045be84427fafc7708e9dac8bd0",
      "c21adf31fe444b15a1b10bb41cd27a90",
      "584ea001d60a433f93ba7b52b474617d",
      "fdfbb63e36ce43738e60ae374ff3cf93",
      "ac5717e157af4b109bb9d60158f2feb5",
      "d5ab7fdd69c84b5ca241787c715dd1fc",
      "8436e3d902db40778d3a90aeb12661f3",
      "6808b5330aaa438a91cf66e1695f8de4",
      "29898fa835f64e9f852a372e9679cd0a",
      "fcf86fdb27c547fb85d84e97a8054174",
      "aeddc8c5f8f24c0cb2d5dc6394612de4",
      "1ede1782131b414a8fed7736f6770b2c",
      "d24244844085449fbac8a02c1a5efc72",
      "6ffd52fc92e7473fa1f014178862f593",
      "c1f6cc7164e74e3dadfd47113c890fa4",
      "7182165f582245d1bcceca303fa2e5f1",
      "b55529cec61b4f2882ca0d1e9ce44f2c",
      "b7d71ec1b11d441fab5380655e893b56",
      "be314e634963485eb8b584f6b6dd6560",
      "73ceec19000d410f8bcf35ca24f31f6d",
      "b00f3b0838ef4a9b80c4e685836fd7ea",
      "94fe55afd2fc4a568d9901b05c6e1af6",
      "436cb34db20d4f889e443ff88d1a68a9",
      "6bf0ac0695af4bc494d39df13e064196",
      "5ed3f82292434da286b381811ab7a6e7",
      "591042122a0f49db97a721108e812ff5",
      "edc1b4006ab940c7a19a73692a792e5d",
      "5ef90a5aae09474f8ade2e399f08c855",
      "8cd93a457ffa4acb980a36e785754d13",
      "19466d9c1aa24348a0b0894ac215eee7",
      "9a05e280b95048e0b57191d31bf89de9",
      "6482a3497cd643ce967c1593a285ffa4",
      "631e7c60a12746269eacb5ef652f0eb9",
      "818434d1853242bf879bf21727c29670",
      "70011eaa8dde4da8a55522004f588665",
      "609c038b1da74f4bbf0b729cd3538fbf",
      "3ff360e3955f4585b74f6ced58f7e76b",
      "744486daa83e4ca2a8c338734514e459",
      "08916884bb4c4fba8ec2b57600bf94a9",
      "264b68fca01d4379821377c35522c26b",
      "adf7fd5e42464e76ac06b2320e05ea1d",
      "71d240ef11df4ab182638b371ba3d0f8",
      "50806cdbe961488eabc7fcc58d67b97f",
      "c97fe64f74cc4baebcab5aa786bfdec4",
      "4be7f18d8bd9409d82ff7941ca47d0f7",
      "760e438bd5484388b44241f3cee95f2f",
      "3dc6d416ec884211bf06648763c184f4",
      "a5dfc458bb524013a6ba57b284f364ef",
      "0b4c2d9e1f784e4ebedb17fb96c5d6bb",
      "d050f2d79e7742c5aa3bc31ae9a20985",
      "6514ea02d95a44e8905ec7efb353e7e7",
      "4db1de0892814ec9a66e342e035f8b9c",
      "3ece7b317ce341c58362a9a32ec99020",
      "9a148c2951564231bfee2ec4aa0be69a",
      "4d6dcf073e574a68834dd02208077dd2",
      "1e891fa51a6248caa0e6a48d7ca3ad10",
      "44906be895ef464d8979536761fc83bd",
      "fc3cb9245f7f4ae980915b9c44029afa",
      "52b2e1bf53904dc7b752a4d39524561e",
      "a22e16fc48764931bdd934ec063eb83f",
      "6fe639ce8bd14c75948e581b2eede041",
      "f7555e8ceb3f412fbca16daef7991260",
      "77a4261dbbd44e2182d0fa877eb34ad0",
      "4985c6650be24ff58d19b5e1c0b6a249",
      "d983683c46ad4b4984ab7a7d087f0ba7",
      "16f074e7284a43c8a530e08d749e0851",
      "8c4326e4d65c401bbde63f888b7447da",
      "6bf9754bda6d4ef384c0ca3b69220474",
      "85ea20fb8676454aa254241ede0a3983",
      "7789be102c294ca6ac4f41fc1144d3b7",
      "e5ab2cce7c344550ae71f78747d95677",
      "3a793bf560c64a81834d9ad9434f630a",
      "f35aa99241824ed5819e6d95277529c4",
      "a22ca4c75a044814a34fb416f5758d8b",
      "907868f50a814c34bbf5a293e290e300",
      "72c5e9ac33074e6791f85ab90c653fbb",
      "ed1888a48b4245fbbf7f6b0725cb5ef0",
      "66775582a3244adfbbdbf9d4ce7d9da7",
      "8126997f81184b8ca703b74c2d0c5a9a",
      "bc201abb1c674d2682735417e5e1fb78",
      "ef70f22720c041b4977e5809a4caf663",
      "579d34bd383d43149f2e1ac85aa0632e",
      "6bb51165cf9f48bf872a0e29e131e101",
      "f98e35fd33af4f03ba162d7e2cf0a998",
      "d3e8d9bb928b4ae5ba3f5a176c7c4d65",
      "cc69ad0db52c4db0a1ce142295e35133",
      "716b7a1162b3447da11071e1733669e3",
      "7c5a155fc7ff4309b9bf97326e4725e5",
      "4694cd18a78f4bb1bdebb7d95b55f2e6",
      "7eb4b574c6dd42369cb737113d3262b0",
      "f5f221edc973477491df08454d863d61",
      "1eb3f06863154ec9bb074a2b8f03dfb2",
      "11978891e5c448769f65aed5f5a5186c",
      "5959f4ee12024079a1624a8be75df377",
      "7236024230324adea53a4cc8afd3a393",
      "05ab4bd8d2d542beb6b350f4d1dc67fe",
      "71ba94b177924c96b8c98423360288af",
      "91076329be6741e99938931a9d76f8ea",
      "a52ca55fbfbd4d5e8cdcae5f0efe9ad0",
      "ee48e3ee15ce4fb2b23fffadb4532d75",
      "92bef56eca94429790bc134232023d3e",
      "e1fa77e8b3cd4346bd31bac3c5963494",
      "817fdcab6c8a452eb9ee2e42a03e5535",
      "4b37b58298d545f7b7da2d669f9adc1e",
      "fa2e70815cf943468577cb672d313096",
      "1af4da9e597e4b8b85eeadc016ace973",
      "398b5d09c3fe42e9a88278f658925158",
      "deef283d76744f48bbaf6cadd4782441",
      "b25c4691a767439d9abf0342e06c7f08",
      "eaf6d318567a41e38c26c5efa7056ac8",
      "8e994fdfde9b46a992254293933586eb",
      "133bae728667482488d2707a4c8c9fa7",
      "c1ddd91427d7418f8ddc54d5b88eb9af",
      "18afd48da46c44d489b8b8979b666887",
      "1950aaf5d66b4eca864c67e9525b790e",
      "430fd512b24f4c338b3b6b47ad23b262",
      "f791ae83dcd64333862ecc7e673cf790",
      "74c796a16aea48a294c19c15a4df9f4d",
      "6effa4b243264a9dbbaca1c51b7e41d1",
      "fb65aa678f6c48b6b46ef2649d2b32cd",
      "841d8e8e7af84808a433ebbe3a857583",
      "f5dec261328548b0b9dc6662c94e8710",
      "969ccee2468244d5b01c49d0beda55bc",
      "4ac7b9b4ea384df7916bbe03e3b7ab14",
      "9c0c8cb095d1483481141125a63d456b",
      "3c6e2ac6123c4a2aa1b11bb890dca1ee",
      "70e00af711fc4bb889e25c330512479f",
      "2c51a859b3e545e4a6ec9e7a50175bad",
      "027975b887a04435b34e9dea92f09e5e",
      "1d6f6838295a49f48cfdf6ff75b47665",
      "8330786b7fdd4666b6fe6f9b6ee08535",
      "0795077e501147ddb91efcb78b19592f",
      "a64258973d9f4a5aadd42a0819b5defb",
      "600f36ea8e3f4d7fb009f5b7bbb6bbe4",
      "687f3734e2e64ab4be2849b4636827d9",
      "7fa39ae255074f2294ee227cba250ed8",
      "3b7b8c92f8dd476794570e55ef537dac",
      "0aeea659e4d14c32af80f33eea105e09",
      "7d3ee423003b4c4e829f66d34a982365",
      "776d91d4f17c4f6f8dfa30fb31e20bd3",
      "684ce7a430da46398c1994d1d302229e",
      "f666ff678e074943ad4d31b92349f433",
      "e4e4bd70ddcb4ac3bf99f5beb87ce010",
      "4a87fd9d817c42afbb9013278186d118",
      "f5f3b029236244a38b805f98c434d965",
      "228e257d0fd24fdbb6586e5963a8d694",
      "393e06996eba45178c5496ff81593632",
      "287439689c2940c384200cfa33dae16c",
      "314c66232ad7406fb0791c1309728123",
      "452433e43c24454789ffe8a9400794d3",
      "fa5c451abfe346668eeb240712d6fe75",
      "4a7ae3666bb14de6be06570af0548960",
      "88fc3663b66c4ad9818c622f9e9a1bab",
      "0d92a442616d4c4c8779bd025ca2e04e",
      "2c5d60aead55442dbc521d9da42d4f25",
      "8a268f7a2cf74438b5ef2f5afb8f13f0",
      "4a0ef6db29af4e1fb294627576c652aa",
      "7107b150f2694ff48147eab6ea25cfcb",
      "9b67d03aa6174d478b7941062d5689e6",
      "e50f642627764fe38d9b8701184cfd0c",
      "8b0583d6ae5344728c37f8c9dbc72725",
      "7bab07816c854f26be4496cb67fb619a",
      "dda5c8a2562344ae951b5dedf2abd842",
      "91805c8f6d0e4bb390ff6a677a0a6b51",
      "d45369aabd10447c9e47a90fee24a5a3",
      "703ba445e9134681bc715e72d9ecf93f",
      "688d2bea64194c5493772c6770e1c8a5",
      "fc498464f1a847f4a9ba288c70d82451",
      "dbf0d6e054854335bb7472d2af0606c5",
      "0761dd3037cb4f999f83fb9c107672ba",
      "0de14984d3fe4c308142a92865c4abd2",
      "27dace60c13a4808b833d3a8a8d6025c",
      "1715d03e613a4c929d10a1d73cdb5fcb",
      "eac5c00887f3460680bcef748d59cb1c",
      "3d0cb4167dfc4cdbb345fd661ef78eea",
      "89c3d04192534c35ba5693a31c32b92b",
      "43036134aa8e4d89b553f262b895cf23",
      "22d4cee7d1574c98ab8c6cab4b653470",
      "ce76ae233e284c1eb410911e2348c43c",
      "6bfdfd50953949909f92a25843dfa07c",
      "a4704843f49f4cdcaddfaeed61414b61",
      "6138ee45df01444ba524510fb26c99fb",
      "2e3a6711b3f540fea09059a07cff389b",
      "2d6e1ca7200642b8b93d06dab75019f7",
      "9afb802a18624fe88d143a5e1132680b",
      "9bc235216faf4d64bf1fea659c9f5856",
      "414a1b1011264ee9a20c887bf1c6ec82",
      "e5c81ea4f1594e3a8c2c7f36517ecd25",
      "bdc626e822f841e0a8a182a00d21b822",
      "dc9cb8a70deb46978278eca66e47d5e2",
      "d1ad2a9dfcd64e349f656b5cfcb98b47",
      "e5e6167c647d455e8d3f62f383401916",
      "2a283aec3e914b749eb575225f7d62c5",
      "30a20ef7db26488b900b335de3d2490d",
      "c2b686698ff24651934180a4edf8fb8c",
      "0f18719b2a69439bb851be3dcc51287c",
      "ba2cbd5f99a74516987dbf41bcbcfeeb",
      "a535c054ceb64a628c8926972d3ed58e",
      "a557b483ecc648ddbd23f1586f8f2416",
      "c4dee437b3144115bc0c28c103e24602",
      "e030a4b3662642c1842ae70aa69d2483",
      "80367b50febc402cafc7f3b2f97bbec6",
      "04db8793825a4a03801cf14232ff1a87",
      "dc70e27428c04366b69dd2c7aaed225c",
      "3b4572f72cd5419c98508a55892aad4e",
      "021e6bc8a3f44c49aafbfc25ee6c45b3",
      "b029e0d426d34dcc9fbf4e53585a9af1",
      "c45ca3e426534790a5bda4ec283237b4",
      "24aa26e6149b49fc973ace96b3d441a8",
      "083b224bd39f4318bf26c9daf9b9efe8",
      "2f7b3b24063b4605bed24dc75354a92d",
      "a85bdc6cda9644bbb2f18d978ac20130",
      "c8dce5bd142140749f5db6ffede2d6ef",
      "ff638bdfb18449119aee0858459f7820",
      "0ea3ac38e5a24529bdfc65fdb236ac9c",
      "03d9961b2d0e4fd9ab43cb9500aa2f67",
      "4c8881c18c094821ae745a67cb0abc5c",
      "8c353f66ac104f00aa180111790098f2",
      "883b13404564492c8b3970c36d990a8b",
      "1fa1ea4de83b4c1d994a941d454ca118",
      "618e3f01006f4c7b8cfda9e43ac6845f",
      "763d6c946b5649c6b30f9e8a0a02df30",
      "fa383eacc40441d5aa2e1de698f1cde8",
      "668059a58b1748a3814a501668cc6ef6",
      "6581402856e64c15b2c43827ab946b0a",
      "b0a69cd03a7f45b9993bf8f1dbe1f8bc",
      "58600d74adda407fbf1dc014c8a4241a",
      "2b7bde43b5514e3f8e5401c6ad04fa37",
      "3b8bb3a762384851bf3b9b5b1de62570",
      "6da7d391eba14550a180b7c32cd4a7a9",
      "64ee921ceef944889d1464703d623276",
      "f5a6bf41849d41b09a9e87b7d57a72c6",
      "6d0e1a6cde414f299cea3ac1bee348bb",
      "ea62531117ea4c458a2cecc262240219",
      "be342bb4736b4b74b7ea118caa472f57",
      "6747dbcea0e145d78910982eb9c77416",
      "d3fc8632eeae490cb8296540e210fadd",
      "82172e86c2dc46fc81c19ce2465cf98c",
      "cf57302635eb4e49b8a02a02302b3c59",
      "c444f197a3fe4b80b6e2a0971a2540e3",
      "7a39a34bd11f4b299d46739f6ea93218",
      "cc10b67dc2954d3a87a63c1a44c41958",
      "d0ca86d8825d4df3a12bb279270bbdd5",
      "86f752d61ac447d9aac80b16190f46a4",
      "e0e9f2a10544434da2b1bdaa5003fc09",
      "7f19d3d7fa2243cc9761e460200b0ffa",
      "6d9afbbffcb841c9a8d3fb5e1720547e",
      "02bf74af122f440684f8708d73834dbb",
      "9956e2ae006d468ca85de2403edf5088",
      "347fc5204f2d4e4f8c9fda5bd0902f21",
      "ef357f1f21af4faf96a78a8df1eaba3a",
      "eb90d250c83e47f98e6d163c4f178da1",
      "4ec7295983434f6e943537aa937298e0",
      "7cc7001b75e54d75871217b6dc52ba9f",
      "a7ac0d184d0f42db839f8c4401cdfdaa",
      "1db32e0da420453182f1ddbfbc1a227a",
      "65387891a1504f9fba52cc59af823e24",
      "1e5c0c55f24f41e48d3a1b8eb7fd1372",
      "c1e234d577224422a799668ba7996638",
      "d68082544bfd484983b7d58910147503",
      "8ad23ad8732a442c92fd105898f68836",
      "f16f20d44ad94b46920acf3c21868942",
      "2ac042e4666a487387f1a054b329119d",
      "a9eab5c9581b4ad9a6a5b793cc7efbcd",
      "dec9be497b564aa290c65328c58643ea",
      "38c60eca711e44b9b834afc08e5b6eb0",
      "4e4f91e722bc4fe09ed25d87e37572d2",
      "dfb62907608f4e12b6a5af467f1047c3",
      "e79542ddb10b4669bf5746d3c318c188",
      "870a7e68935f4085a1856634dda750b2",
      "d97dfcb109b8497aa018c906ee5caf76",
      "72a4f723d7a94ebf8eb656d0a7c5f897",
      "f34dac1d380f426da4219e70dfbba824",
      "664ba1b590714a2bbb2b4b926ccbb820",
      "22fc4450dc9b46e5b7c462389be2ef33",
      "ec33df77eed2498e9b76875f7ce6b7ae",
      "7de6fae82c6b4b94b5f2c07cea738433",
      "6e6b79045a9a4bbba7edb1837a7e41a2",
      "4a70de8980b7430aa9bdd45a27668917",
      "0cfe9cd385eb43238e37553d3d744ba1",
      "20ca68d805274103afbf846062cc0c7b",
      "f30b92dc23be4cdbb7d3958bf68b28c3",
      "662932eb4e6148cdba437a72b8343e17",
      "4ac1e096d2534237b72411864611dcca",
      "24df86a48db145f28dc4061b04906956",
      "9105d8b66cd14f2b8f42121e76e7036c",
      "89fa6819e8dd4b4e898283b0ee911e63",
      "461226d148f6488180b7ab8b16e28ab3",
      "9ed5e0021d6c43c2b1c273116d4aa2cf",
      "c467eb1f848f4fa8a771522447473418",
      "c74dda5cdcdf4d5bbbb78658c37f9c2e",
      "6ded0967ed7a408385cb0068e4d9a048",
      "a22d5973ac2a4b12bedd3d64dc7c1ae1",
      "15ce760388944117acf204d6ea00a949",
      "4a10f4cbd2434b6cb84d177f62f4f1db",
      "d3c3f5a3e91445c99678f4ce7a673105",
      "d2c8d22e590644f28a2331c7871cc2bc",
      "8d25322ce888473b9004659fad44a73b",
      "d4fd80388e9849bf95b8670148e98780",
      "1651196da7db4988861972454c08553e",
      "98df91f250a34fa8b157a06c1bb13a86",
      "0875e32380364cbd82cb50807785a15f",
      "85d55c4d5c17426db6e040b58cf479b2",
      "1870bfe972894726953cd986f0885964",
      "b09e1374ab0e4613b4badaa562a92bc7",
      "c43828e226334b4ea020e52963896c37",
      "83b8187047f449d39d9168c5d8f86718",
      "e950bfc52aa84d198187a0c13264c8b2",
      "8b69135d26d9444d876c096362168b0f",
      "a5c4478652074478938b7d9e4b67610c",
      "121386345d10491da186a7d7e24acb30",
      "80815e6fc1f1475f9b118cbfad715212",
      "97ac7b9a166a4696a9fb3ba38adb43b9",
      "da6c57dc71134489868b616e7adcd27d",
      "eb2ab2a0f1e74650919630f9cc190b1a",
      "d7cfa9ad76c4425c9af1b6f1cd15ded8",
      "3473b5c2069448bf821104574d5cfed3",
      "8aff72e20bc3460ca880e179df5013c1",
      "42460cdc0135482e837c3e4017e82ea8",
      "92d831e5b7ad4faaa9a605f348fbbe61",
      "de6fb2035dc14a7c96b6edf85b592d89",
      "fbd6b80b24324bbd94f928866eb87d52",
      "9dccf6236fe948989475bd1c51cc9956",
      "84ec6e3ae1d34842bdefcef0abb18654",
      "4b34530c424144e79ac11d22b7ca21e1",
      "fc4c39bd65aa464fb26254f0cec8f570",
      "bc5d095dfab54db4aa17181ed165bc9b",
      "89e801157a0f4a8897632a3a13546248",
      "1c000e52a6dc470d9ddecf23045d3381",
      "f802b37b37d84a65a62c9e00e2dce011",
      "5afb5dc118fb4d6db7bb925212dcd93a",
      "42660ca64985469aa3f4190e2f81330c",
      "b0edc9c54d75408dbaf74090f8f18226",
      "feb114794cad4715850afece82145998",
      "e23ed0ce0e3d49bd8cc0f066d7a4c89a",
      "3662883fb86c4117bdaecc452829baad",
      "aec698a79da0442b80cc5c4c140b2f08",
      "a1cc8a6667544217b2da45206d6f6808",
      "718bbe9efd6f47e9881d7756b149a48e",
      "5d7df31ee3b94cfcb335a5b8c6644044",
      "4511953c2eff4bca9d21297fcaad4504",
      "36aea6f661a64b9f9e1d6ffb8078b313",
      "7cc3b0ec3dd049068450ab04fa7c4483",
      "ae77fc9d5da54c3788b7cfadb6950703",
      "240026e37cd848b3b3e74a7da4a3a791",
      "f6c8dc4e6ad049308656c2a0cac588a5",
      "93490ba30e1848769dbfea5c50a5e07f",
      "9c92c0f5a666433e9873a47ebd44d130",
      "afd1e74034354f869536fffe289669a8",
      "67603b74d8c943af9398a75e1b2a83ab",
      "7c672073a9944893bceb8a5b1640e0fd",
      "4a308717c1424767ad28733746669133",
      "4f5ee9b5452947219d908b10e0191081",
      "783fc98f17ef434ba8995376d29b790a",
      "997519ffdfe0428796e993c9e346160c",
      "1c2bfe634e4d41c78f3c147fe6d0d407",
      "fe19ee6cda384370ac5cbfb5a61d9624",
      "dd9c33eb5108426bb54f05925564c5fd",
      "2ff1ac0ffe8349babda36d879e3fdf15",
      "5262b6c2fbce4984b8ef4fab83a93adf",
      "9d01ff738c0b47fe88af4783d5ef9506",
      "843e7fdaf43a45d583f4396a170877db",
      "647f8b1e0b64474ba7f54bb364127b6d",
      "bee2231f0711412bac189aa0af07023c",
      "605ac1971aa64f9aa67e35a859694f7a",
      "85f900f5377d401fb330653062128ffb",
      "d84140960c54421e8bd30577e8777356",
      "ed181026efb64413991c19448ab91970",
      "394b7355e60b48a5afdfef5f64f35be4",
      "42313c5defac498e83e77e4e34d3bbd9",
      "56a816c24cd14cd4932492d675fca516",
      "8116a6ac48d945d5b54af1ba9f8c6439",
      "caff3179f2e54cdb81337016e0bb923c",
      "621a350d944143c88e47e789bd1e0b2d",
      "a6f4c513a6844ed491cdb31a10582d67",
      "97d7017f89a1426abd16258ff6ef3e33",
      "684f4738032e41b49d28b2699f0a4379",
      "57e53c9a254045689a4418acf3921b04",
      "240e00ff43494fbcb30b37055e7f990b",
      "b4774ad2e01c447b9e21d3a16f39cb38",
      "e6b2bd2a04b84f199b64e1ea3749febc",
      "3895441c88bd4aba9fefbe0381990592",
      "ddb005cb82ba4cf9a5d16e8b95290e70",
      "0d604b228a0d4e95ad4d89e2b88c53f0",
      "1e1104ced27f491a94e0423ff0868da8",
      "b4f0b9210c3348518871806c9a27bf3f",
      "9f26fb23b94540dd97b523c1f5e6a689",
      "1cd1c259f12a461294775b66b3f5941b",
      "b664b56fef664afd9d34d7c4f66fc6fd",
      "45206179f43f4ef6b95f666278c1d3f8",
      "e0e16ab5741a419daecf2374839c7502",
      "d95f7f8fdefd4d72acbec88aa2d1239c",
      "524566608973471cb799eaaaa0351510",
      "3e99c08cdd1241e6970952026348545a",
      "41e210e5553d4e009c532746f56f8818",
      "0e5042c397f041dabc67ed9d26a1c24d",
      "b6b6893184a94fd1975a7640f65477ad",
      "e838fdbaabba45938c630cae5ac74f65",
      "abcaec4a055744bf87a33f10a50f39a3",
      "ef8a0bc77f864f77988ed14e5b53e0fd",
      "fb780b2f7de94ac89d6291e95b264685",
      "678600bec0694a0388555296683e049b",
      "44a1cc48e0664bac8afc2862b85e0f9e",
      "9a58ddc667e14c8697af3e3e168403b6",
      "b9ba81e51989452b9d7feeed80846686",
      "8f32c33128fd43e2aded6ba9779044d5",
      "c1ccfcb736824466b12aad6223c2bbe2",
      "60aee5edc1f94823bd39ccf38c374ea0",
      "f827d55149ee4eaf9ee13375e254bd84",
      "fce34333376a4428afc7ff526ddccc03",
      "422283e26cfd4ad199a01cade4269ad1",
      "2ed0ac5cc626494497fc6c762d10ebb8",
      "e9d1a6e160d946a6b016b2f5817dab7b",
      "3a43cec969bb46a5a72658166388f14d",
      "ba70354b62664f9b9fefe6a6f1668173",
      "ff64e01719bf45fd9f19bed3d28cbaef",
      "559f730e0bbe4d10b920737795d64a35",
      "e143edad7eee4b2a92667619f420331c",
      "6ab4138e43eb4ac2ac57628f365d9885",
      "226ce31127394308b87ab4fbbfa0766c",
      "c6f739ee4850478ead53c42ac7bfb6d1",
      "e5efd578de21484e9033d39f2a6d82de",
      "ffde2beda2ea4e5987112ddc9cb9e5c2",
      "23ccb43af71143079055b24b7a46a9dd",
      "e0713356d95e44eeb9317a658799fdf1",
      "8c1e7577deca4bca87f2722733e3a2ab",
      "aab55d39ca8549ddb0df9f5a7eb0cf56",
      "eedb1c1ec0b147eba1dcec56d0e29568",
      "ea0ebbf8684242e4a8e7503138bfe12b",
      "5f752a825c384f448e221430759479ed",
      "0d547b2920c3411b868cf3982b92a326",
      "f5e2931e00ee4b7388ab8312b52f9c46",
      "a4d9c02306fb4539a6abbdf5e221ac11",
      "b91a604c770b467c8c9c176f1a34084b",
      "60bf9e82649c44569c17e91202975779",
      "5f235a7342ae418c9e38d65c97e81f77",
      "1812e66c66314b59ac096a84aad79571",
      "a82075e62a304467b6c074993547ae3b",
      "df8a11178fb44220a868c77f62111a9a",
      "f4eebd31af5f4b8e9233d82b573d6b9b",
      "2797345e5b36435aadff9843f2cde791",
      "83f3886f54664ac7b0727fa6c8be6119",
      "5838eeb5a2b949a69f78fc16fdeccb2b",
      "2264e901488448f0826b600f5f211cb1",
      "e2af7ba561da4e5280712f88c490ba43",
      "599d91a459a1487fbf1e3063a49d163f",
      "c4b1b425d82f4e86a7f332834a851218",
      "2829d755382d401a960a68b53a957da4",
      "06c4139a076d4d419a357fc7d967ba84",
      "a5d7f380156c4ec2b10f309bf1aca2eb",
      "6c954bbf51564c92b72ba1dc96d0342b",
      "70031585f770451abb557db3d1948422",
      "538085bfd253456280a1ba7c9b93dc35",
      "4c07af2caef84cac8a14b127b99411d0",
      "0c7d2e324ae84565bb4c7a87cc6820e8",
      "73ed005ebec4474f9ebd81f08ed6e3e1",
      "d37e9abe5803455ea10fed3a648e29d3",
      "f201bcc47546457c8d6b985eeb47f24b",
      "fcb0b301b3c7404783219a2362fd0241",
      "1224cb054ae94909848f6d84426acecf",
      "94af706fe8d74859bb73a4af4fe007b5",
      "081886204d8c45ebb6c1490283446f63",
      "313e87ebf31f48788aa57f4d67cd01f6",
      "9f3a0e54b910462385f77d220826011e",
      "0e3da423fc564a1c8c1823cac4aba948",
      "b48ca96b7087463da7d0ff38b8531e8e",
      "9cafc4dc0bfc43d7b6c4d0d81bd611c8",
      "d465c268a1114cfda00b52ee92c497cb",
      "85d12d8df40d4462ae3a048640ed6485",
      "bf564d9de2fe45dd8631d9f13f981410",
      "fe02a7145c2a4bf8b0248c342192c26f",
      "034553112cd4471ebf80d74eea01b383",
      "dac07e4926ce4f74b6adc931a029b181",
      "1a919e58d20144f7902d0b79c06f675f",
      "dcfd3417a4084d3fb7f9542ebdf7c317",
      "fcb5ac89d7644f19a4c2a20ce982e022",
      "9b4b3dd6a22444b9a0ad337accc0da4a",
      "ba5bb394e6a441ca9677fa393a008627",
      "5bb204ede7dc49b78f4ffcf81d98b6af",
      "2066cb969c3649b9acb188c16c9ae41b",
      "9bc1c5225bf0431f9e36627ce5522a27",
      "f2b7f9e7fe6e4d88991592f12e160068",
      "1bc0285f813d4758b8cc760c5b5f834a",
      "49279306f7254938ab527368c683c0b3",
      "d528c6e538424549b5157f664d6e8502",
      "5addb5d7bbf6451eb29a102d00aa62f1",
      "916627fddb414f21b0c73640adfb45c8",
      "5f192ba679f943e8b188ecf5131d9472",
      "fa2af519600e4135af0c33a694588c2a",
      "f38d5b3675284815b9ad7abc973ad80b",
      "461100a5fae54694a1e5ee56f0828568",
      "d301cfb9da52481e94c9848693162e76",
      "1894420c8f324f55b71447147df7e21f",
      "9659df1df42e4006ad8caa0b0341f001",
      "ba91346a89894a7cb51f0721a42e7d48",
      "bcc33e73d6cc4bae879858d7a1055b8c",
      "a6a3e023382a48eb80341b68589df8c8",
      "ffbbf2ea348c42cfb78cff0ab3c1e22a",
      "2fab1a3a1d2b4600850e3459662a4c38",
      "a762fad1ca314de5be1b7e24092b9542",
      "792abed45ba44c40bfa3685e867d23bd"
     ]
    },
    "id": "-4wi-MbgMPJS",
    "outputId": "4e88f6f9-db22-45f1-8a99-4c694e6406c0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11ba92bc69df4acb8aa1fc33aed0a794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/46 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# Define the model ID for the large 20B parameter model.\n",
    "model_id = \"EleutherAI/gpt-neox-20b\"\n",
    "\n",
    "# Create a BitsAndBytesConfig with NF4 quantization and bfloat16 compute dtype, optimized for large models.\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "   # bnb_4bit_compute_dtype=torch.bfloat32\n",
    ")\n",
    "\n",
    "# Load the tokenizer for the large model.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Load the large model with the specified 4-bit quantization configuration.\n",
    "# `device_map=\"auto\"` is crucial here to distribute the model layers across available devices.\n",
    "model_4bit = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=\"auto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FOjmNADVaGdw",
    "outputId": "6819ca38-9c29-44aa-9617-93a4bbeb90a0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'action': '10K_DOCUMENT_QUERY',\n",
       " 'reason': 'Query related to annual reports',\n",
       " 'answer': 'Access through document database'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "def get_response_json(text: str, user_query: str) -> str:\n",
    "    \"\"\"\n",
    "    Return the JSON string that immediately follows the example:\n",
    "        - User: \"<user_query>\"\n",
    "      and the 'Response:' line.\n",
    "    \"\"\"\n",
    "    # Look for \"Strictly follow this format\" and find the first JSON object after that\n",
    "    strict_pattern = r\"Strictly follow this format for every query, and never deviate\\.\"\n",
    "    strict_match = re.search(strict_pattern, text)\n",
    "\n",
    "    if not strict_match:\n",
    "        raise ValueError(\"Could not find 'Strictly follow this format' marker\")\n",
    "\n",
    "    # Search for JSON object starting from after the strict pattern\n",
    "    search_text = text[strict_match.end():]\n",
    "\n",
    "    # JSON object pattern that handles nested braces and quoted strings\n",
    "    json_pattern = r'\\{(?:[^{}\"]|\"[^\"\\\\]*(?:\\\\.[^\"\\\\]*)*\")*\\}'\n",
    "\n",
    "    json_match = re.search(json_pattern, search_text)\n",
    "    if not json_match:\n",
    "        raise ValueError(f'No JSON object found after \"Strictly follow this format\" marker')\n",
    "\n",
    "    return json_match.group(0)\n",
    "\n",
    "def get_response_json_as_dict(text: str, user_query: str) -> dict:\n",
    "    \"\"\"Same as above, but parsed into a Python dict.\"\"\"\n",
    "    return json.loads(get_response_json(text, user_query))\n",
    "\n",
    "def route_query_quantized(user_query: str):\n",
    "    router_system_prompt =f\"\"\"\n",
    "    As a professional query router, your objective is to correctly classify user input into one of three categories based on the source most relevant for answering the query:\n",
    "    1. \"OPENAI_QUERY\": If the user's query appears to be answerable using information from OpenAI's official documentation, tools, models, APIs, or services (e.g., GPT, ChatGPT, embeddings, moderation API, usage guidelines).\n",
    "    2. \"10K_DOCUMENT_QUERY\": If the user's query pertains to a collection of documents from the 10k annual reports, datasets, or other structured documents, typically for research, analysis, or financial content.\n",
    "    3. \"INTERNET_QUERY\": If the query is neither related to OpenAI nor the 10k documents specifically, or if the information might require a broader search (e.g., news, trends, tools outside these platforms), route it here.\n",
    "\n",
    "    Your decision should be made by assessing the domain of the query.\n",
    "\n",
    "    Always respond in this valid JSON format:\n",
    "    {{\n",
    "        \"action\": \"OPENAI_QUERY\" or \"10K_DOCUMENT_QUERY\" or \"INTERNET_QUERY\",\n",
    "        \"reason\": \"brief justification\",\n",
    "        \"answer\": \"AT MAX 5 words answer. Leave empty if INTERNET_QUERY\"\n",
    "    }}\n",
    "\n",
    "    EXAMPLES:\n",
    "\n",
    "    - User: \"How to fine-tune GPT-3?\"\n",
    "    Response:\n",
    "    {{\n",
    "        \"action\": \"OPENAI_QUERY\",\n",
    "        \"reason\": \"Fine-tuning is OpenAI-specific\",\n",
    "        \"answer\": \"Use fine-tuning API\"\n",
    "    }}\n",
    "\n",
    "    - User: \"Where can I find the latest financial reports for the last 10 years?\"\n",
    "    Response:\n",
    "    {{\n",
    "        \"action\": \"10K_DOCUMENT_QUERY\",\n",
    "        \"reason\": \"Query related to annual reports\",\n",
    "        \"answer\": \"Access through document database\"\n",
    "    }}\n",
    "\n",
    "    - User: \"Top leadership styles in 2024\"\n",
    "    Response:\n",
    "    {{\n",
    "        \"action\": \"INTERNET_QUERY\",\n",
    "        \"reason\": \"Needs current leadership trends\",\n",
    "        \"answer\": \"\"\n",
    "    }}\n",
    "\n",
    "    - User: \"What's the difference between ChatGPT and Claude?\"\n",
    "    Response:\n",
    "    {{\n",
    "        \"action\": \"INTERNET_QUERY\",\n",
    "        \"reason\": \"Cross-comparison of different providers\",\n",
    "        \"answer\": \"\"\n",
    "    }}\n",
    "\n",
    "    Strictly follow this format for every query, and never deviate.\n",
    "    User: \"{user_query}\"\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Query the GPT-4 model with the router prompt and user input\n",
    "\n",
    "        device = \"cuda:0\"\n",
    "       \n",
    "        # Encode the input text using the tokenizer and move the resulting tensors to the specified device.\n",
    "        inputs = tokenizer(router_system_prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        # Generate a sequence of tokens from the model based on the input.\n",
    "        # `max_new_tokens` limits the length of the generated text.\n",
    "\n",
    "        \n",
    "        outputs = model_4bit.generate(**inputs, max_new_tokens=200)\n",
    "\n",
    "        # Decode the generated token IDs back into human-readable text and print the output.\n",
    "\n",
    "        task_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return get_response_json_as_dict(task_response,  user_query)\n",
    "\n",
    "\n",
    "    # Handle case where model response isn't valid JSON\n",
    "    except json.JSONDecodeError as json_err:\n",
    "        return {\n",
    "            \"action\": \"INTERNET_QUERY\",\n",
    "            \"reason\": f\"JSON parsing error: {json_err}\",\n",
    "            \"answer\": \"\"\n",
    "        }\n",
    "\n",
    "    # Catch-all for any other unforeseen issues\n",
    "    except Exception as err:\n",
    "        return {\n",
    "            \"action\": \"INTERNET_QUERY\",\n",
    "            \"reason\": f\"Unexpected error: {err}\",\n",
    "            \"answer\": \"\"\n",
    "        }\n",
    "\n",
    "route_query_quantized(\"what is the revenue of uber in 2021?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The router using the quantized model seems to be working. So let's upda the agentic_rag below to see if it matches to openai's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note \n",
    "After some experimentation it is difficult to use a quantized LLM at least for the ones I have tried to get good generated responses in RAG. Therefore we are using for this notebook to use quantized models just for the routing. We will continue to use OpenAI for the RAG responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agentic_rag_quantized(user_query: str):\n",
    "    \"\"\"\n",
    "    Main function that runs the full Agentic RAG system.\n",
    "\n",
    "    This function takes a user's question, decides what type of query it is (OpenAI-related,\n",
    "    financial document-related, or general internet), and then calls the right function\n",
    "    to handle it. Finally, it prints out the full conversation and response.\n",
    "\n",
    "    Args:\n",
    "        user_query (str): The user's input question.\n",
    "\n",
    "    Returns:\n",
    "        None (It just prints the result nicely to the console)\n",
    "    \"\"\"\n",
    "\n",
    "    #  Terminal color codes to make the printed output easier to read and visually structured\n",
    "    CYAN = \"\\033[96m\"\n",
    "    GREY = \"\\033[90m\"\n",
    "    BOLD = \"\\033[1m\"\n",
    "    RESET = \"\\033[0m\"\n",
    "\n",
    "    try:\n",
    "        # Step 1: Print the user's original question to the console\n",
    "        print(f\"{BOLD}{CYAN}üë§ User Query:{RESET} {user_query}\\n\")\n",
    "\n",
    "        # Step 2: Use the router (powered by GPT) to decide which route the query belongs to\n",
    "        try:\n",
    "            response = route_query_quantized(user_query)\n",
    "        except Exception as route_err:\n",
    "            # If something goes wrong while classifying the query, show an error message\n",
    "            print(f\"{BOLD}{CYAN}ü§ñ BOT RESPONSE:{RESET}\\n\")\n",
    "            print(f\"Routing error: {route_err}\\n\")\n",
    "            return\n",
    "\n",
    "        # Extract the routing decision and the reason behind it\n",
    "        action = response.get(\"action\")  # e.g., \"OPENAI_QUERY\"\n",
    "        reason = response.get(\"reason\")  # e.g., \"Related to OpenAI tools\"\n",
    "\n",
    "        # Step 3: Show the selected route and why it was chosen\n",
    "        print(f\"{GREY}üìç Selected Route: {action}\")\n",
    "        print(f\"üìù Reason: {reason}\")\n",
    "        print(f\"‚öôÔ∏è Processing query...{RESET}\\n\")\n",
    "\n",
    "        # Step 4: Call the correct function depending on the route (retrieval or web search)\n",
    "        try:\n",
    "            route_function = routes.get(action)  # Find the function to use for this route\n",
    "            if route_function:\n",
    "                result = route_function(user_query, action)  # Run the function with the user's input\n",
    "            else:\n",
    "                result = f\"Unsupported action: {action}\"  # Catch unknown routing types\n",
    "        except Exception as exec_err:\n",
    "            result = f\"Execution error: {exec_err}\"  # Handle failure in the chosen route function\n",
    "\n",
    "        # Step 5: Print the final response to the user\n",
    "        print(f\"{BOLD}{CYAN}ü§ñ BOT RESPONSE:{RESET}\\n\")\n",
    "        print(f\"{result}\\n\")\n",
    "\n",
    "    except Exception as err:\n",
    "        # Catch-all for any unexpected errors in the overall logic\n",
    "        print(f\"{BOLD}{CYAN}ü§ñ BOT RESPONSE:{RESET}\\n\")\n",
    "        print(f\"Unexpected error occurred: {err}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[96müë§ User Query:\u001b[0m what was uber revenue in 2021?\n",
      "\n",
      "\u001b[90müìç Selected Route: 10K_DOCUMENT_QUERY\n",
      "üìù Reason: Revenue details in 10k filings\n",
      "‚öôÔ∏è Processing query...\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[96mü§ñ BOT RESPONSE:\u001b[0m\n",
      "\n",
      "Uber's revenue in 2021 was $17,455 million [1].\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agentic_rag(\"what was uber revenue in 2021?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[96müë§ User Query:\u001b[0m what was lyft revenue in 2024?\n",
      "\n",
      "\u001b[90müìç Selected Route: INTERNET_QUERY\n",
      "üìù Reason: Future financial data search\n",
      "‚öôÔ∏è Processing query...\u001b[0m\n",
      "\n",
      "Getting your response from the internet üåê ...\n",
      "\u001b[1m\u001b[96mü§ñ BOT RESPONSE:\u001b[0m\n",
      "\n",
      "Lyft revenue in 2024 was $5.786B, a 31.39% increase from 2023.\n",
      "\n",
      "Here are additional details and sources confirming Lyft's 2024 revenue:\n",
      "\n",
      "- Lyft's Q4 and full-year 2024 revenue increased to $1.6 billion and $5.79 billion respectively, up 27% and 31% year-over-year.\n",
      "- According to an article in Business of Apps, Lyft generated $5.7 billion revenue in 2024, a 31.3% percent year-on-year increase.\n",
      "- Another article in Pymnts.com mentioned Lyft's full-year revenue rose 31%, to $5.8 billion in 2024.\n",
      "- In an article in Yahoo Finance, Lyft's 2024 revenues were reported to be $5.79 billion with a 31% growth compared to the previous year.\n",
      "\n",
      "The sources of this information are as follows:\n",
      "- \"Lyft Revenue 2017-2025\" on Macrotrends\n",
      "- \"Lyft Reports Record Q4 and Full-Year 2024 Results\" on Investor_lyft\n",
      "- \"Lyft Revenue and Usage Statistics (2025)\" on Business of Apps\n",
      "- An article titled \"Lyft's 2024 Success Driven by Customer Focus, Price Lock Service\" on Pymnts.com\n",
      "- An article in Yahoo Finance titled \"Lyft Full Year 2024 Earnings: EPS Beats Expectations\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agentic_rag(\"what was lyft revenue in 2024?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[96müë§ User Query:\u001b[0m List me down new LLMs in 2025\n",
      "\n",
      "\u001b[90müìç Selected Route: INTERNET_QUERY\n",
      "üìù Reason: Needs information beyond current data\n",
      "‚öôÔ∏è Processing query...\u001b[0m\n",
      "\n",
      "Getting your response from the internet üåê ...\n",
      "\u001b[1m\u001b[96mü§ñ BOT RESPONSE:\u001b[0m\n",
      "\n",
      "**Latest LLMs in 2025**\n",
      "\n",
      "You can find the latest LLMs in 2025 by checking the following models:\n",
      "1. GPT-4.5 (Orion)\n",
      "2. Claude 3.7 Sonnet\n",
      "3. Gemini 2.5 Pro\n",
      "4. DeepSeek-V3-0324\n",
      "5. Grok-3\n",
      "6. Qwen3\n",
      "7. Llama 4\n",
      "8. LLaMA 3\n",
      "9. OpenAI\n",
      "10. Mistral\n",
      "11. Llama 4 Scout\n",
      "12. Llama 4 Maverick\n",
      "\n",
      "**References:**\n",
      "- Splunk: Top LLMs To Use in 2025: Our Best Picks (https://www.splunk.com/en_us/blog/learn/llms-best-to-use.html)\n",
      "- NetApp Instaclustr: Top 10 open source LLMs for 2025 (https://www.instaclustr.com/education/open-source-ai/top-10-open-source-llms-for-2025/)\n",
      "- Shakudo: Top 9 Large Language Models as of August 2025 (https://www.shakudo.io/blog/top-9-large-language-models)\n",
      "- TechTarget: 27 of the best large language models in 2025 (https://www.techtarget.com/whatis/feature/12-of-the-best-large-language-models)\n",
      "- Backlinko: 23 Best Large Language Models (LLMs) in 2025 (https://backlinko.com/list-of-llms)\n",
      "- Codingscape: Most powerful LLMs (Large Language Models) in 2025 (https://codingscape.com/blog/most-powerful-llms-large-language-models)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agentic_rag(\"List me down new LLMs in 2025\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[96müë§ User Query:\u001b[0m how to work with chat completions?\n",
      "\n",
      "\u001b[90müìç Selected Route: OPENAI_QUERY\n",
      "üìù Reason: Chat completions relate to OpenAI\n",
      "‚öôÔ∏è Processing query...\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[96mü§ñ BOT RESPONSE:\u001b[0m\n",
      "\n",
      "To work with chat completions, there are several components that you need to be aware of::\n",
      "\n",
      "1. Retrieving Chat Messages: You can use the OpenAI API to retrieve chat messages for a specific completion ID. The request for this will look similar to this: \n",
      "```\n",
      "curl https://api.openai.com/v1/chat/completions/chat_abc123/messages \\\n",
      "  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n",
      "  -H \"Content-Type: application/json\"\n",
      "```\n",
      "You can also retrieve a specified number of messages and sort their order by timestamp in ascending or descending order [1].\n",
      "\n",
      "2. Listing stored chat completions: You can list the stored chat completions with a GET request. Only chat completions that have the store parameter set to true will be returned. You can also filter these by various metadata [1].\n",
      "\n",
      "3. Retrieve a Specific Chat Completion: If you know the specific completion_id, you can retrieve it directly. An example request for that would look like this: \n",
      "```\n",
      "curl https://api.openai.com/v1/chat/completions/chatcmpl-abc123 \\\n",
      "  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n",
      "  -H \"Content-Type: application/json\"\n",
      "```\n",
      "This would return a ChatCompletion object matching the specified ID [1].\n",
      "\n",
      "4. Streaming Chat Completions: The OpenAI API also provides a streaming feature where you can get chunks of completions returned from the model in real-time using server-sent events. Each chunk of a chat completion will have the object type set to \"chat.completion.chunk\" [1].\n",
      "\n",
      "By using these features, you can effectively interact with and manage chat completions using the OpenAI API.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agentic_rag(\"how to work with chat completions?\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "11ba92bc69df4acb8aa1fc33aed0a794": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_1f395f30d33d4ee0ba0696d01ba92268",
        "IPY_MODEL_5bf5b4eb2214451f87ae1e6dee67a759",
        "IPY_MODEL_c4324831de86441dbe15a3ed5ca24295"
       ],
       "layout": "IPY_MODEL_76d6b06337f54f8c80dd9aff4625824a"
      }
     },
     "1f395f30d33d4ee0ba0696d01ba92268": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_b7fc510b6e534e43a8010953664b6327",
       "style": "IPY_MODEL_3ec14fd639d547268998704497afce44",
       "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
      }
     },
     "3ec14fd639d547268998704497afce44": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "47c11600d1c74fde9c13b46015c83dc4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "5bf5b4eb2214451f87ae1e6dee67a759": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_c2f94b92593d48799f5bb3906cafa29d",
       "max": 46,
       "style": "IPY_MODEL_b8b354a2a0a546ec91865e34907ff7c9",
       "value": 46
      }
     },
     "76d6b06337f54f8c80dd9aff4625824a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b7fc510b6e534e43a8010953664b6327": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b8b354a2a0a546ec91865e34907ff7c9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "c2f94b92593d48799f5bb3906cafa29d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c4324831de86441dbe15a3ed5ca24295": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_fc5012a2e83f48c9862fbb72c9b48780",
       "style": "IPY_MODEL_47c11600d1c74fde9c13b46015c83dc4",
       "value": "‚Äá46/46‚Äá[02:23&lt;00:00,‚Äá‚Äá1.35s/it]"
      }
     },
     "fc5012a2e83f48c9862fbb72c9b48780": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
